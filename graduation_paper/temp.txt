Fi的计算开销可以表示为Ks=wis + outnode +msgnum，（解释一下）
等式
上述等式也同样表达了，针对一轮计算，某个fragment的执行时间取决以下三个要素,wis,为本轮计算的active nodes数目，代表了需要执行多少次点中心计算，
outnode，是本fragment外部点的数目，代表需要传递消息的数量，msgnum是本fragment接收到的消息数，上述等式采用了简单的求和形式主要有一下几方面原因，
1) 接收到的消息数目通常与wis成线性关系 2) 每轮计算上述三个过程，即接收消息、计算、发送消息之间串行相互独立 3）消息的发送时间（包括消息的序列化时间）与
消息的size成线性关系,因此上述三要素也同样指导我们针对不同类别图算法提取不同的特征

问题定义
首先我们将运行时间 t_i 的预测定义成回归问题：对于某一计算节点 Pi 及给定的运行时信息 X_i  ，我们需要训练出一个预测模型 P ，用于评估下一轮迭代的运行时间 τ ，因为该预测模型的输出是以毫秒为单位的实数，因此该问题可以定义成机器学习中的回归问题，具体方程如下：
τ=P(X_i)
因此预测过程分为以下两个部分：1) 收集运行时信息 X_i  ，相对于机器学习中的特征提取。一个理想的信息〖 X〗_i  应足以描述下一轮程序的运行状态；2) 针对候选回归模型进行分析、设计并训练出一个适合的预测器。需要特别指出，由于估值阶段在图计算过程中仅执行一次，因此我们只针对接下来的迭代增量阶段进行运行时间与消息到达速率的预测。

约束
除了机器学习领域中过拟合与预测的准确性外，图计算领域还关心额外几个指标：a) 预测时间约束，在迭代预测过程中，我们需保证预测时间足够短并尽可能减少预测开销，如果一次预测占用了迭代计算的大部分时间，那么此次预测没有任何意义；b) 训练数据约束，某些图应用算法中，可能无法采取线下训练的方式，必须依靠迭代的线上时间完成训练预测等工作，这需要我们的模型在小训练数据集上有较好的准确性；c) 超参数约束，虽然我们针对众多图应用算法采用不同的训练预测思路，但我们更希望针对多样化的数据输入给出一个通用的计算模型。

模型选取
1 局部加权线性回归模型：从历史数据分析得出，线性回归模型不适用于针对图计算中运行时间 t_i  及消息到达速率〖 s〗_i  的预测，相反，往往采用一条类似于二次函数的曲线可对数据拟合的更好，而线性模型很容易出现欠拟合现象，不能取得最好的预测效果，但是局部加权线性回归模型允许给待预测点附近的每个点赋予一定的权重，从而在估计中引入一些偏差来降低预测的均方误差，最终解决了在非线性模型中建立线性模型的问题，因此比较适合图计算中运行时间与消息速率的预测。
2 岭回归模型：由于岭回归模型的对于简单问题的性能优势、并可以处理特征数多于样本数的情况，因此它特别适合图计算中运行时间与消息速率的预测。不仅如此，岭回归模型还有如下优势：(1) 在岭回归模型的预测过程中最多只需要几十次积累操作，因此其满足图计算的时间性能要求；(2) 虽然其计算复杂度为 O(n^3 )，但其只依赖于特征向量的维度，因此对于图计算预测中特征向量维度或样本数据较小的情形下具有一定优势。
3 随机森林模型：在随机森林模型中，包含了多个回归树，其中每个回归树构建时的样本都是由训练集经过有放回抽样得来的，并进行独立的预测。最终预测结果由各回归树投票决定。随机森岭模型不仅在高维度离散的数据数据下表现出很好的性能，其在以下方面同样特别适合图计算中运行时间与消息速率的预测：(1) 对于一个包含 n 个回归树的随机森林，其中所有回归树的平均深度是 d ，最好与最坏情况下，其预测的复杂度分别是是 O(n log⁡d )和 O(nd)，因此其满足图计算得时间性能要求；(2) [23]实验证明，大多数随机森林模型在训练复杂度上更趋近于最好情况而不是最坏情况，因此这为图计算的线上训练提供了可能性；(3) 由于最终的预测取决于所有回归树的投票，因此对于训练数据的轻微变动具有很好的适应性，这为图计算预测的正确定提供了保证；(4) 该模型只有一个超参数，即回归树的个数，因此该模型针对不同类别的图算法可能更具有通用型。
4 逐步线性回归
5 神经网络

特征提取
上述分析，

损失函数

回归预测（不同算法）
